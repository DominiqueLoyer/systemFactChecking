#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mini Benchmark LIAR (Mode Light)
=================================
Teste SysCRED sur un Ã©chantillon du dataset LIAR sans ML.
Utilise uniquement les heuristiques et rÃ¨gles symboliques.

(c) Dominique S. Loyer - PhD Thesis Prototype
"""

import sys
import os
import time
import random
from collections import Counter

# Path setup
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '02_Code/syscred'))

from liar_dataset import LIARDataset, LiarStatement, LiarLabel

# ============================================
# HEURISTIQUES SIMPLES (Sans ML)
# ============================================

# Mots indicateurs de dÃ©sinformation
FAKE_INDICATORS = [
    "hoax", "fake", "conspiracy", "unbelievable", "shocking",
    "they don't want you to know", "secret", "cover up", "scam",
    "miracle", "cure", "100%", "guaranteed", "never before",
    "breaking:", "exposed", "bombshell"
]

# Mots indicateurs de sources fiables
CREDIBLE_INDICATORS = [
    "according to", "research shows", "study found", "data indicates",
    "scientists say", "experts", "report", "analysis", "percent",
    "university", "institute", "published"
]

# Sources connues fiables
CREDIBLE_SPEAKERS = [
    "barack-obama", "hillary-clinton", "john-boehner", "nancy-pelosi"
]

# Sources Ã  faible crÃ©dibilitÃ© historique
LOW_CRED_SOURCES = [
    "chain-email", "bloggers", "viral-image", "facebook-posts"
]


def analyze_statement_heuristic(stmt: LiarStatement) -> dict:
    """
    Analyse une dÃ©claration avec des heuristiques simples.
    Retourne un score de 0.0 Ã  1.0.
    """
    text = stmt.statement.lower()
    
    # Scores partiels
    fake_score = 0.0
    cred_score = 0.0
    
    # 1. DÃ©tecter les indicateurs de fake news
    for indicator in FAKE_INDICATORS:
        if indicator in text:
            fake_score += 0.15
    
    # 2. DÃ©tecter les indicateurs de crÃ©dibilitÃ©
    for indicator in CREDIBLE_INDICATORS:
        if indicator in text:
            cred_score += 0.1
    
    # 3. Analyser le speaker
    speaker = stmt.speaker.lower() if stmt.speaker else ""
    if speaker in LOW_CRED_SOURCES:
        fake_score += 0.3
    elif speaker in CREDIBLE_SPEAKERS:
        cred_score += 0.15
    
    # 4. PrÃ©sence de nombres (signe de factualitÃ©)
    import re
    if re.search(r'\d+\.?\d*%?', text):
        cred_score += 0.1
    
    # 5. Longueur (les vrais Ã©noncÃ©s sont souvent plus dÃ©taillÃ©s)
    if len(text) > 150:
        cred_score += 0.05
    
    # 6. Ponctuation excessive (indicateur de sensationnalisme)
    if text.count('!') > 1 or text.count('?') > 2:
        fake_score += 0.1
    
    # Calcul du score final (0.5 = neutre)
    base_score = 0.5
    final_score = base_score + cred_score - fake_score
    final_score = max(0.0, min(1.0, final_score))
    
    return {
        'score': final_score,
        'fake_indicators': fake_score,
        'cred_indicators': cred_score,
        'predicted_binary': 'Real' if final_score >= 0.5 else 'Fake'
    }


def run_mini_benchmark(sample_size: int = 100):
    """ExÃ©cute un mini-benchmark sur un Ã©chantillon."""
    
    print("=" * 60)
    print("ğŸ§ª SysCRED - Mini Benchmark LIAR (Mode Light)")
    print("=" * 60)
    
    # Charger le dataset
    dataset_path = os.path.join(os.path.dirname(__file__), '02_Code/syscred/datasets/liar')
    ds = LIARDataset(dataset_path)
    test_data = ds.load_split('test')
    
    # Ã‰chantillon alÃ©atoire
    sample = random.sample(test_data, min(sample_size, len(test_data)))
    print(f"\nğŸ“Š Benchmark sur {len(sample)} dÃ©clarations\n")
    
    # RÃ©sultats
    results = []
    correct_binary = 0
    
    start_time = time.time()
    
    for i, stmt in enumerate(sample):
        analysis = analyze_statement_heuristic(stmt)
        
        is_correct = analysis['predicted_binary'] == stmt.binary_label
        if is_correct:
            correct_binary += 1
        
        results.append({
            'id': stmt.id,
            'statement': stmt.statement[:60],
            'ground_truth': stmt.binary_label,
            'predicted': analysis['predicted_binary'],
            'score': analysis['score'],
            'correct': is_correct
        })
        
        # Afficher quelques exemples
        if i < 5:
            emoji = "âœ…" if is_correct else "âŒ"
            print(f"{emoji} [{stmt.binary_label:4s}â†’{analysis['predicted_binary']:4s}] {stmt.statement[:50]}...")
            print(f"   Score: {analysis['score']:.2f}")
    
    elapsed = time.time() - start_time
    
    # Calcul des mÃ©triques
    accuracy = correct_binary / len(sample) * 100
    
    # MÃ©triques par classe
    true_positives = sum(1 for r in results if r['ground_truth'] == 'Fake' and r['predicted'] == 'Fake')
    false_positives = sum(1 for r in results if r['ground_truth'] == 'Real' and r['predicted'] == 'Fake')
    false_negatives = sum(1 for r in results if r['ground_truth'] == 'Fake' and r['predicted'] == 'Real')
    true_negatives = sum(1 for r in results if r['ground_truth'] == 'Real' and r['predicted'] == 'Real')
    
    precision_fake = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall_fake = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_fake = 2 * precision_fake * recall_fake / (precision_fake + recall_fake) if (precision_fake + recall_fake) > 0 else 0
    
    # Afficher les rÃ©sultats
    print("\n" + "=" * 60)
    print("ğŸ“ˆ RÃ‰SULTATS DU BENCHMARK")
    print("=" * 60)
    
    print(f"\nğŸ“Š MÃ©triques globales:")
    print(f"   Accuracy:     {accuracy:.1f}%")
    print(f"   Correct:      {correct_binary}/{len(sample)}")
    print(f"   Temps total:  {elapsed:.2f}s")
    print(f"   Temps/stmt:   {elapsed/len(sample)*1000:.1f}ms")
    
    print(f"\nğŸ“Š MÃ©triques pour classe 'Fake':")
    print(f"   Precision:    {precision_fake:.2f}")
    print(f"   Recall:       {recall_fake:.2f}")
    print(f"   F1-Score:     {f1_fake:.2f}")
    
    print(f"\nğŸ“Š Matrice de confusion:")
    print(f"                  PrÃ©dit")
    print(f"                  Fake    Real")
    print(f"   RÃ©el  Fake     {true_positives:4d}    {false_negatives:4d}")
    print(f"         Real     {false_positives:4d}    {true_negatives:4d}")
    
    # Analyse des erreurs
    print(f"\nğŸ“Š Analyse des erreurs:")
    errors = [r for r in results if not r['correct']]
    print(f"   Total erreurs: {len(errors)}")
    
    if errors:
        print("\n   Exemples d'erreurs:")
        for err in errors[:5]:
            print(f"   - [{err['ground_truth']}â†’{err['predicted']}] {err['statement']}")
    
    # Baseline comparaison
    print(f"\nğŸ“Š Comparaison avec baseline:")
    print(f"   Random guess:  50.0%")
    print(f"   Majority vote: {max(sum(1 for s in sample if s.binary_label=='Real'), sum(1 for s in sample if s.binary_label=='Fake'))/len(sample)*100:.1f}%")
    print(f"   SysCRED (light): {accuracy:.1f}%")
    
    improvement = accuracy - 50.0
    print(f"\n   â­ AmÃ©lioration vs random: +{improvement:.1f}%")
    
    return {
        'accuracy': accuracy,
        'precision_fake': precision_fake,
        'recall_fake': recall_fake,
        'f1_fake': f1_fake,
        'sample_size': len(sample)
    }


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--sample', type=int, default=200, help='Taille de l\'Ã©chantillon')
    args = parser.parse_args()
    
    run_mini_benchmark(args.sample)
