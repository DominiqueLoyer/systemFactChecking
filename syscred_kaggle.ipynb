{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SysCRED - Système Neuro-Symbolique de Vérification de Crédibilité\n",
                "\n",
                "**PhD Thesis Prototype** - Dominique S. Loyer  \n",
                "*Citation Key: loyerModelingHybridSystem2025*\n",
                "\n",
                "Ce notebook intègre:\n",
                "- Moteur de recherche TREC (BM25, QLD, TF-IDF)\n",
                "- Analyse NLP avec Transformers (GPU/TPU)\n",
                "- Ontologie RDF pour l'explicabilité\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration de l'Environnement Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Vérification GPU/TPU ===\n",
                "import torch\n",
                "import subprocess\n",
                "import sys\n",
                "import os\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"SysCRED - Kaggle Environment Check\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "    DEVICE = 'cuda'\n",
                "else:\n",
                "    print(\"✗ No GPU - using CPU\")\n",
                "    DEVICE = 'cpu'\n",
                "\n",
                "# Check for TPU (Kaggle)\n",
                "try:\n",
                "    import torch_xla.core.xla_model as xm\n",
                "    print(f\"✓ TPU available\")\n",
                "    DEVICE = xm.xla_device()\n",
                "except:\n",
                "    pass\n",
                "\n",
                "print(f\"\\nDevice: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Installation des dépendances ===\n",
                "!pip install -q pyserini transformers rdflib pytrec_eval nltk beautifulsoup4 python-whois\n",
                "\n",
                "# NLTK resources\n",
                "import nltk\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "\n",
                "print(\"✓ Dépendances installées\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Modules SysCRED"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === IR Engine (extrait de TREC_AP88-90) ===\n",
                "import re\n",
                "import math\n",
                "from typing import Dict, List, Tuple, Optional, Any\n",
                "from dataclasses import dataclass\n",
                "from collections import Counter\n",
                "\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "@dataclass\n",
                "class SearchResult:\n",
                "    doc_id: str\n",
                "    score: float\n",
                "    rank: int\n",
                "    snippet: Optional[str] = None\n",
                "\n",
                "class IREngine:\n",
                "    \"\"\"Moteur IR avec BM25, QLD, TF-IDF (Citation: loyerEvaluationModelesRecherche2025)\"\"\"\n",
                "    \n",
                "    BM25_K1 = 0.9\n",
                "    BM25_B = 0.4\n",
                "    \n",
                "    def __init__(self, use_stemming: bool = True):\n",
                "        self.stopwords = set(stopwords.words('english'))\n",
                "        self.stemmer = PorterStemmer() if use_stemming else None\n",
                "        self.searcher = None\n",
                "    \n",
                "    def preprocess(self, text: str) -> str:\n",
                "        \"\"\"Prétraitement avec stemming Porter.\"\"\"\n",
                "        if not isinstance(text, str):\n",
                "            return \"\"\n",
                "        tokens = word_tokenize(text.lower())\n",
                "        filtered = [t for t in tokens if t.isalpha() and t not in self.stopwords]\n",
                "        if self.stemmer:\n",
                "            filtered = [self.stemmer.stem(t) for t in filtered]\n",
                "        return ' '.join(filtered)\n",
                "    \n",
                "    def calculate_bm25(self, query_terms: List[str], doc_terms: List[str],\n",
                "                       doc_length: int, avg_doc_length: float,\n",
                "                       doc_freq: Dict[str, int], corpus_size: int) -> float:\n",
                "        \"\"\"Calcul BM25.\"\"\"\n",
                "        doc_counts = Counter(doc_terms)\n",
                "        score = 0.0\n",
                "        for term in query_terms:\n",
                "            if term not in doc_counts:\n",
                "                continue\n",
                "            tf = doc_counts[term]\n",
                "            df = doc_freq.get(term, 1)\n",
                "            idf = math.log((corpus_size - df + 0.5) / (df + 0.5) + 1)\n",
                "            num = tf * (self.BM25_K1 + 1)\n",
                "            den = tf + self.BM25_K1 * (1 - self.BM25_B + self.BM25_B * doc_length / avg_doc_length)\n",
                "            score += idf * (num / den)\n",
                "        return score\n",
                "    \n",
                "    def pseudo_relevance_feedback(self, query: str, top_docs: List[str], n_terms: int = 10) -> str:\n",
                "        \"\"\"Expansion de requête par PRF.\"\"\"\n",
                "        query_tokens = self.preprocess(query).split()\n",
                "        expansion = Counter()\n",
                "        for doc in top_docs:\n",
                "            for token in self.preprocess(doc).split():\n",
                "                if token not in query_tokens:\n",
                "                    expansion[token] += 1\n",
                "        expansion_terms = [t for t, _ in expansion.most_common(n_terms)]\n",
                "        return query + ' ' + ' '.join(expansion_terms)\n",
                "\n",
                "# Test\n",
                "ir = IREngine()\n",
                "print(\"Test preprocess:\", ir.preprocess(\"Information Retrieval systems help find documents\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SEO Analyzer ===\n",
                "class SEOAnalyzer:\n",
                "    \"\"\"Analyse SEO avec TF-IDF, BM25, PageRank estimé.\"\"\"\n",
                "    \n",
                "    BM25_K1 = 1.5\n",
                "    BM25_B = 0.75\n",
                "    \n",
                "    STOPWORDS = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
                "                 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
                "                 'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'et', 'ou'}\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.avg_doc_length = 500\n",
                "        self.corpus_size = 1000\n",
                "    \n",
                "    def tokenize(self, text: str) -> List[str]:\n",
                "        if not text:\n",
                "            return []\n",
                "        tokens = re.findall(r'\\b[a-zA-Z]{2,}\\b', text.lower())\n",
                "        return [t for t in tokens if t not in self.STOPWORDS]\n",
                "    \n",
                "    def calculate_tf_idf(self, text: str) -> Dict[str, float]:\n",
                "        tokens = self.tokenize(text)\n",
                "        if not tokens:\n",
                "            return {}\n",
                "        counts = Counter(tokens)\n",
                "        total = len(tokens)\n",
                "        tf_idf = {}\n",
                "        for term, count in counts.items():\n",
                "            tf = count / total\n",
                "            idf = math.log(self.corpus_size / (1 + len(term)))  # Simplified IDF\n",
                "            tf_idf[term] = tf * idf\n",
                "        return tf_idf\n",
                "    \n",
                "    def estimate_pagerank(self, domain_age_days: int = None, source_reputation: str = None) -> float:\n",
                "        d = 0.85\n",
                "        base = 1 - d\n",
                "        contrib = 0\n",
                "        if domain_age_days and domain_age_days > 365*5:\n",
                "            contrib += 0.3\n",
                "        elif domain_age_days and domain_age_days > 365*2:\n",
                "            contrib += 0.2\n",
                "        if source_reputation == 'High':\n",
                "            contrib += 0.3\n",
                "        elif source_reputation == 'Medium':\n",
                "            contrib += 0.15\n",
                "        return min(1.0, base + d * contrib)\n",
                "\n",
                "# Test\n",
                "seo = SEOAnalyzer()\n",
                "print(\"TF-IDF top terms:\", sorted(seo.calculate_tf_idf(\"credibility verification system\").items(), \n",
                "                                  key=lambda x: x[1], reverse=True)[:3])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. NLP avec Transformers (GPU/TPU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Chargement des modèles NLP ===\n",
                "from transformers import pipeline\n",
                "\n",
                "print(\"Chargement des modèles NLP sur\", DEVICE, \"...\")\n",
                "\n",
                "# Sentiment Analysis\n",
                "sentiment_analyzer = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "    device=0 if DEVICE == 'cuda' else -1\n",
                ")\n",
                "\n",
                "# Named Entity Recognition\n",
                "ner_model = pipeline(\n",
                "    \"ner\",\n",
                "    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
                "    aggregation_strategy=\"simple\",\n",
                "    device=0 if DEVICE == 'cuda' else -1\n",
                ")\n",
                "\n",
                "print(\"✓ Modèles chargés\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Test NLP ===\n",
                "test_text = \"According to researchers at Harvard University, the new study published in Nature shows significant results.\"\n",
                "\n",
                "print(\"Text:\", test_text)\n",
                "print(\"\\nSentiment:\", sentiment_analyzer(test_text)[0])\n",
                "print(\"\\nEntities:\")\n",
                "for ent in ner_model(test_text):\n",
                "    print(f\"  - {ent['word']}: {ent['entity_group']} ({ent['score']:.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Système de Vérification de Crédibilité"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SysCRED - Système complet ===\n",
                "from datetime import datetime\n",
                "\n",
                "class SysCRED:\n",
                "    \"\"\"Système neuro-symbolique de vérification de crédibilité.\"\"\"\n",
                "    \n",
                "    WEIGHTS = {\n",
                "        'source_reputation': 0.25,\n",
                "        'domain_age': 0.10,\n",
                "        'sentiment_neutrality': 0.15,\n",
                "        'entity_presence': 0.15,\n",
                "        'coherence': 0.15,\n",
                "        'fact_check': 0.20\n",
                "    }\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.ir_engine = IREngine()\n",
                "        self.seo_analyzer = SEOAnalyzer()\n",
                "        self.sentiment = sentiment_analyzer\n",
                "        self.ner = ner_model\n",
                "    \n",
                "    def verify(self, text: str, source_reputation: str = 'Unknown', domain_age_days: int = 0) -> Dict:\n",
                "        \"\"\"Vérifier la crédibilité d'un texte.\"\"\"\n",
                "        scores = {}\n",
                "        \n",
                "        # 1. Source reputation\n",
                "        rep_map = {'High': 1.0, 'Medium': 0.6, 'Low': 0.3, 'Unknown': 0.5}\n",
                "        scores['source_reputation'] = rep_map.get(source_reputation, 0.5)\n",
                "        \n",
                "        # 2. Domain age\n",
                "        if domain_age_days > 365*5:\n",
                "            scores['domain_age'] = 1.0\n",
                "        elif domain_age_days > 365*2:\n",
                "            scores['domain_age'] = 0.7\n",
                "        elif domain_age_days > 365:\n",
                "            scores['domain_age'] = 0.5\n",
                "        else:\n",
                "            scores['domain_age'] = 0.3\n",
                "        \n",
                "        # 3. Sentiment neutrality\n",
                "        sent = self.sentiment(text[:512])[0]\n",
                "        confidence = sent['score']\n",
                "        # Neutral = high credibility, extreme = lower\n",
                "        scores['sentiment_neutrality'] = 1 - abs(confidence - 0.5) * 2\n",
                "        \n",
                "        # 4. Entity presence (sources, institutions)\n",
                "        entities = self.ner(text[:512])\n",
                "        credible_entities = sum(1 for e in entities if e['entity_group'] in ['ORG', 'PER'])\n",
                "        scores['entity_presence'] = min(1.0, credible_entities * 0.2)\n",
                "        \n",
                "        # 5. Text coherence (based on preprocessing)\n",
                "        preprocessed = self.ir_engine.preprocess(text)\n",
                "        unique_ratio = len(set(preprocessed.split())) / max(1, len(preprocessed.split()))\n",
                "        scores['coherence'] = unique_ratio\n",
                "        \n",
                "        # 6. Fact check (placeholder - would use external API)\n",
                "        scores['fact_check'] = 0.5  # Neutral by default\n",
                "        \n",
                "        # Calculate weighted score\n",
                "        overall = sum(scores[k] * self.WEIGHTS[k] for k in self.WEIGHTS)\n",
                "        \n",
                "        # Determine level\n",
                "        if overall >= 0.7:\n",
                "            level = 'HIGH'\n",
                "        elif overall >= 0.4:\n",
                "            level = 'MEDIUM'\n",
                "        else:\n",
                "            level = 'LOW'\n",
                "        \n",
                "        return {\n",
                "            'score': round(overall, 3),\n",
                "            'level': level,\n",
                "            'components': {k: round(v, 3) for k, v in scores.items()},\n",
                "            'sentiment': sent,\n",
                "            'entities': entities[:5],\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        }\n",
                "\n",
                "# Initialize\n",
                "syscred = SysCRED()\n",
                "print(\"✓ SysCRED initialized\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Test SysCRED ===\n",
                "test_texts = [\n",
                "    {\n",
                "        'text': \"According to a study published by Harvard University in the journal Science, researchers have discovered a new method for detecting misinformation.\",\n",
                "        'source': 'High',\n",
                "        'age': 3650\n",
                "    },\n",
                "    {\n",
                "        'text': \"SHOCKING!!! You won't BELIEVE what scientists found! This changes EVERYTHING!!!\",\n",
                "        'source': 'Unknown',\n",
                "        'age': 30\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"SysCRED - Tests de Vérification\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, test in enumerate(test_texts, 1):\n",
                "    print(f\"\\n--- Test {i} ---\")\n",
                "    print(f\"Text: {test['text'][:80]}...\")\n",
                "    \n",
                "    result = syscred.verify(\n",
                "        text=test['text'],\n",
                "        source_reputation=test['source'],\n",
                "        domain_age_days=test['age']\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nScore: {result['score']} ({result['level']})\")\n",
                "    print(\"Components:\")\n",
                "    for k, v in result['components'].items():\n",
                "        print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Intégration TREC (Optional - si dataset disponible)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Vérification du dataset TREC ===\n",
                "KAGGLE_TREC_PATH = \"/kaggle/input/trec-ap88-90\"  # Ajuster selon votre dataset\n",
                "\n",
                "import os\n",
                "if os.path.exists(KAGGLE_TREC_PATH):\n",
                "    print(f\"✓ TREC Dataset found: {KAGGLE_TREC_PATH}\")\n",
                "    !ls -la {KAGGLE_TREC_PATH}\n",
                "else:\n",
                "    print(\"✗ TREC Dataset not found\")\n",
                "    print(\"Pour utiliser l'indexation TREC:\")\n",
                "    print(\"1. Créer un dataset Kaggle avec AP.tar\")\n",
                "    print(\"2. L'ajouter comme input à ce notebook\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Ontologie RDF (Explicabilité)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Sauvegarde des résultats en RDF ===\n",
                "from rdflib import Graph, Namespace, Literal, URIRef\n",
                "from rdflib.namespace import RDF, RDFS, XSD\n",
                "\n",
                "CRED = Namespace(\"http://example.org/credibility#\")\n",
                "\n",
                "def save_to_ontology(result: Dict, text_id: str) -> Graph:\n",
                "    \"\"\"Convertir un résultat de vérification en triplets RDF.\"\"\"\n",
                "    g = Graph()\n",
                "    g.bind('cred', CRED)\n",
                "    \n",
                "    eval_uri = CRED[f\"Evaluation_{text_id}\"]\n",
                "    \n",
                "    g.add((eval_uri, RDF.type, CRED.CredibilityEvaluation))\n",
                "    g.add((eval_uri, CRED.hasScore, Literal(result['score'], datatype=XSD.float)))\n",
                "    g.add((eval_uri, CRED.hasLevel, Literal(result['level'], datatype=XSD.string)))\n",
                "    g.add((eval_uri, CRED.timestamp, Literal(result['timestamp'], datatype=XSD.dateTime)))\n",
                "    \n",
                "    for comp_name, comp_value in result['components'].items():\n",
                "        comp_uri = CRED[f\"{text_id}_{comp_name}\"]\n",
                "        g.add((eval_uri, CRED.hasComponent, comp_uri))\n",
                "        g.add((comp_uri, CRED.componentName, Literal(comp_name)))\n",
                "        g.add((comp_uri, CRED.componentScore, Literal(comp_value, datatype=XSD.float)))\n",
                "    \n",
                "    return g\n",
                "\n",
                "# Test\n",
                "g = save_to_ontology(result, \"test_001\")\n",
                "print(f\"Triplets RDF générés: {len(g)}\")\n",
                "print(g.serialize(format='turtle')[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Notes\n",
                "\n",
                "- **GPU/TPU**: Ce notebook utilise automatiquement le GPU/TPU Kaggle disponible\n",
                "- **TREC**: Pour utiliser le dataset TREC, uploadez AP.tar comme dataset Kaggle\n",
                "- **Citation**: loyerModelingHybridSystem2025, loyerEvaluationModelesRecherche2025"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}